

<p align="center">
  <img src="docs/assets/Roaya_VL_logo_w256.png" width="220" alt="Roaâ€™ya-VL logo">
</p>

<p align="center" style="color:#4b5563; margin-top:-10px; font-size:20px; font-weight:800;">
 Ù…Ø´Ø±ÙˆØ¹ Ø±Ø¤ÙŠØ§ â€” Ø¥Ø·Ø§Ø± Ø¹Ù…Ù„ Ø¹Ù…Ù„ÙŠ Ù„Ø¨Ù†Ø§Ø¡ ÙˆØªÙ‚ÙŠÙŠÙ… Ù†Ù…Ø§Ø°Ø¬ Ù„ØºÙˆÙŠØ©-Ø¨ØµØ±ÙŠØ© Ø¹Ø±Ø¨ÙŠâ€“Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ Ù„ÙÙ‡Ù… Ø§Ù„ØµÙˆØ± ÙˆÙ‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù†ØµÙˆØµ (OCR) ÙˆØ§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„.
  ÙŠÙ‡Ø¯Ù Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¥Ù„Ù‰ ØªÙˆØ«ÙŠÙ‚ Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª Ø¹Ø¨Ø± Ù…Ø±Ø§Ø­Ù„ Ø¨Ù†Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ VLM: Ù…Ø§ Ù‚Ø¨Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨/Ø§Ù„Ù…Ø­Ø§Ø°Ø§Ø©ØŒ Ø§Ù„Ø¶Ø¨Ø· Ø§Ù„Ø®Ø§Ø¶Ø¹ Ù„Ù„Ø¥Ø´Ø±Ø§Ù (SFT)ØŒ
  Ø«Ù… ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª (DPO/ORPO/GRPO) Ø£Ùˆ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªØ¹Ø²ÙŠØ²ÙŠ</p>

<h1 align="center" style="font-size:42px; line-height:1.15; margin:10px 0 8px;">
  Roaâ€™ya-VL-3B: Compression-First Visual Tokenization for Arabicâ€“English VLMs
</h1>

<p align="center">
  <b>Yakoub Bazi</b><sup>1</sup> Â·
  <b>Mansour Zuair</b><sup>1</sup> Â·
  <b>Mohamad Mahmoud Al Rahhal</b><sup>2</sup>
</p>

<p align="center" style="color:#4b5563; font-size:14px; line-height:1.5; margin-top:-6px;">
  <sup>1</sup>Computer Engineering Department, College of Computer and Information Sciences, King Saud University, Riyadh 11543, Saudi Arabia<br/>
  <span style="font-family:ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace;">
    {ybazi, zuair}@ksu.edu.sa
  </span>
  <br/><br/>
  <sup>2</sup>Applied Computer Science Department, College of Applied Computer Science, King Saud University, Riyadh 11543, Saudi Arabia<br/>
  <span style="font-family:ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace;">
    malrahhal@ksu.edu.sa
  </span>
</p>

<p align="center">
  <a href="#" target="_blank">
    <img alt="Paper" src="https://img.shields.io/badge/Paper-soon-6c757d?style=for-the-badge">
  </a>
  <a href="https://yakoubbazi.github.io/Roaya-VL/" target="_blank">
    <img alt="Project Page" src="https://img.shields.io/badge/Project-Page-2563eb?style=for-the-badge">
  </a>
  <a href="https://github.com/yakoubbazi/Roaya-VL" target="_blank">
    <img alt="Code" src="https://img.shields.io/badge/Code-GitHub-111827?style=for-the-badge">
  </a>
  <a href="#" target="_blank">
    <img alt="Models" src="https://img.shields.io/badge/HuggingFace-soon-f59e0b?style=for-the-badge">
  </a>
</p>

---

## ğŸ”¥ Status
**Stage-2 (instruction tuning) completed up to ~50K steps.**  
- Data mixture size: **~18.0M samples** (loaded: **17,937,575**)  
- Batch size: **128**  
- Selected **best checkpoint: 45K** (used as the Stage-2 model for reporting and next stages)

---

## ğŸ“¢ Latest Updates
- **2026-01-21**: Added Stage-2 validation trajectory figure (**MMBench-EN / OCRBench / TextVQA / Avg**).
- **2026-01-27**: Selected **best Stage-2 checkpoint (45K)** from the trajectory (trained up to **50K**).

---

## ğŸ§± Training pipeline (current: Stage 2)
<p align="center">
  <img src="docs/assets/Train_Pipeline.png" width="900" alt="Roaâ€™ya-VL training pipeline (Stage 2)">
</p>
<p align="center"><i>Stage-2 instruction tuning: DeepSeek-OCRâ€“inspired compression-first visual tokenization + projector + Qwen2.5-VL-3B backbone, trained on a FineVision mix with Arabic.</i></p>

---

## ğŸ“ˆ Stage-2 validation trajectory (reported)

<p align="center">
  <img src="docs/assets/stage2_trends_en.png" width="1100" alt="Stage-2 validation trajectory (MMBench-EN / OCRBench / TextVQA / Average)">
</p>
<p align="center"><i>Validation trend across Stage-2 checkpoints (MMBench-EN, OCRBench, TextVQA, and Average). We report the best checkpoint (~45K) as the final Stage-2 model.</i></p>

---

## What is Roaâ€™ya-VL-3B?
We introduce **Roaâ€™ya-VL-3B**, a bilingual **Arabicâ€“English** visionâ€“language model (VLM) built to evaluate whether **compression-first, OCR-style visual tokenization** can generalize beyond OCR to broader VLM tasks under a **fixed visual token budget**.

Roaâ€™ya-VL-3B combines:
- A **DeepSeek-OCR** visual encoder (compression-first tokenization; SAM & CLIP features)
- A lightweight **projector**
- A strong open backbone **Qwen2.5-VL-3B**

The model supports token-efficient regimes (256/400 tokens) and document-scale tiling (up to 9Ã—400 tokens). We study practical training stages (alignment â†’ instruction tuning â†’ Arabic consolidation â†’ preference optimization) and provide transparent intermediate validation to understand how **data mixture**, **tokenization**, and **Arabic-focused consolidation** affect OCR fidelity, reasoning, and Arabic visual understanding.

---

## Tokenization regimes
- **Tiny:** 520Ã—520 â†’ **96** visual tokens  
- **Small:** 680Ã—680 â†’ **100** visual tokens  
- **Base:** 1024Ã—1024 â†’ **256** visual tokens  
- **Large:** 1280Ã—1280 â†’ **400** visual tokens  
- **Tiling:** up to **9Ã—400** tokens for document-scale inputs / multi-image  

---

## Resources
- **Project page:** https://yakoubbazi.github.io/Roaya-VL/
- **Code:** https://github.com/yakoubbazi/Roaya-VL
- **Paper:** soon
- **Models:** soon

---

## Roadmap
- [x] Stage-2 instruction tuning to **~50K steps**
- [x] Pick **best checkpoint (~45K)** based on Stage-2 validation trend
- [ ] Expanded evaluation on additional benchmarks (InfoVQA, POPE, MMMU, MMStar, etc.)
- [ ] **Stage-2.5 Arabic consolidation** (Arabic instruction + OCR/doc + culture)
- [ ] Stage-3 **preference optimization** (DPO / ORPO / GRPO) when preference/reward data is ready
- [ ] Teaser examples (OCR / Doc / VQA / multi-image)
- [ ] Reproducibility checklist (scripts + configs)
- [ ] Public release (weights + code + evaluation)

---

## Acknowledgement
- **LLaVA-NeXT**: the codebase we built upon for training/evaluation utilities and core VLM engineering components.  
  https://github.com/LLaVA-VL/LLaVA-NeXT

- **DeepSeek-OCR**: the OCR-style, compression-first vision encoder inspiration we adopt/adapt for token-efficient visual tokenization in Roaâ€™ya-VL.  
  https://github.com/deepseek-ai/DeepSeek-OCR

- **VLMEvalKit**: the evaluation toolkit we used to integrate Roaâ€™ya-VL and run standardized evaluations across multiple VLM benchmarks.  
  https://github.com/open-compass/VLMEvalKit

> **Note:** Roaâ€™ya-VL is an independent research project and is not affiliated with LLaVA-NeXT, DeepSeek-OCR, or VLMEvalKit.

---

## Citation
```bibtex
@article{bazi2025roaya,
  title   = {Roa'ya-VL-3B: Compression-First Visual Tokenization for Arabic-English VLMs},
  author  = {Bazi, Yakoub and Zuair, Mansour and Al Rahhal, Mohamad Mahmoud},
  journal = {arXiv preprint arXiv:XXXX.XXXXX},
  year    = {2026}
}
